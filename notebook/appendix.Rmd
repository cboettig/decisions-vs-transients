---
output: github_document
---


```{r setup, message=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
library(tidyverse) 
library(MDPtoolbox)
library(mdplearning)
library(expm)
library(truncnorm)

```

# Outbreak model

- 'harvest' term corresponds to removal of pest, with associated cost
- also experience damage costs proportional to pest abundance

```{r}
#damage <- 0.3
#control <- 100
#discount <- 0.99

damage <- 0.05
control <- 1
discount <- 0.98

n_s <- 110
states <- seq(0,110, length=n_s)
actions <- states

p <- list(r = .8, K = 153, q = 2, b = 20, sigma = .02, a=2.3, x0 = 20)
eps <- states[2] / 10

may <- function(a){  
  function(x, h = 0){ # May
    pmax(
      x + x * p$r * (1 - x / p$K)  - a * x ^ p$q / (x ^ p$q + p$b ^ p$q) + eps - h,  
      0)
  }
}

reward_fn <- function(x,h) - damage * (x / 100) - control * h / 100


#reward_fn <- function(x,h) - damage * (x / max(states)) - control * h / max(actions) * x / max(states)

```

Range of possible a that covers tipping in both directions.  Belief is in a stable system while the reality is a transient (smaller `a`).

```{r}

true_a <- 27.4      ### 21.5
believe_a <- 29 # 28.5   ### 22.5

possible_a <- seq(2*true_a - believe_a, 2*believe_a - true_a, by = 0.2)
true_i <- which.min(abs(possible_a - true_a))
```

```{r}

## True a: The Ghost Attractor
names(possible_a) <- possible_a
df <- map_dfr(possible_a, 
        function(a){
          f <- may(a)
          tibble(x = states,
                 f = f(x,0) - x) 
        },
        .id = "a") %>% 
  mutate(group = a, 
         a = as.numeric(a))
        


df %>%
  ggplot(aes(x, f)) + 
    geom_point(aes(col = a), alpha = 0.1) + 
    geom_line(aes(lty = group), data = df %>% filter(a == true_a | a == believe_a), 
              lwd = 1, show.legend = FALSE) +
    geom_hline(aes(yintercept = 0)) 

```



```{r}

transition_matrix <- function(states, actions, f, sigma){
    n_s <- length(states)
    n_a <- length(actions)
    transition <- array(0, dim = c(n_s, n_s, n_a))
    for(i in 1:n_a){
      for (k in 1:n_s) {
          nextpop <- f(states[k], actions[i])
          if (nextpop < 0) {
          #if(which.min(abs(states - nextpop)) > 1) {
             x  <- c(1, rep(0, n_s - 1))
          } else {
            x <- truncnorm::dtruncnorm(states, 0, max(states), nextpop, sigma * nextpop)
            if(sum(x) <= 0){ 
              x  <- c(1, rep(0, n_s - 1))
            } else {
              x <- x / sum(x)
            }
          }
          transition[k, , i] <- x
      }
  }
  if(any(is.na(transition))) stop("error creating transition matrix")
    transition
}
```



```{r}
m_true <- transition_matrix(states, actions, may(true_a), p$sigma)
m_belief <- transition_matrix(states, actions, may(believe_a), p$sigma)
```


"simulate" without any actions


```{r}
x0 <- which.min(abs(states - p$x0))
Tmax <- 200
```


```{r}
X <- numeric(length(states))
X[2] <- 1

prob_dynamics <- function(M, X, Tmax){
  probability <- t(M) %^% Tmax %*% X 
  data.frame(state = states, probability)
}
  

prob_dynamics(m_true[,,1], X, Tmax) %>% 
  ggplot(aes(state,probability))  + geom_point() # + geom_bar(stat="identity")
```


```{r}

## `actions` is an argument of indices, not necessarily the action values themselves 
sim <- function (transition,  x0, Tmax, action = rep(1, Tmax)){
    n_states <- dim(transition)[2]
    state <- numeric(Tmax + 1)
    state[1] <- x0
    time <- 1:Tmax
    for (t in time) {
        state[t + 1] <- base::sample(1:n_states, 
                               1, 
                               prob = transition[state[t], , action[t]])
    }
    data.frame(time = 1:Tmax, state = state[time])
}
```



```{r}
set.seed(12346)
no_switches <- sim(m_true, x0, Tmax)  %>% 
  mutate(state = states[state])

no_switches %>% 
  ggplot(aes(time, state)) + geom_point() + geom_path() 
```

```{r}
set.seed(12345)
switches <- sim(m_true, x0, Tmax) %>% mutate(state = states[state])

switches  %>% 
  ggplot(aes(time, state)) + geom_point() + geom_path() 
```





```{r}
set.seed(1234)
stable <- sim(m_belief, x0, Tmax)  %>% 
  mutate(state = states[state])

stable %>% 
  ggplot(aes(time, state)) + geom_point() + geom_path() 
```





```{r}
## Collect the data sets.  Use names.
examples <- bind_rows(switches = switches, 
                      no_switches = no_switches, 
                      stable = stable, 
                      .id="series") 
```




----

# Learning

```{r}
x0 <- which.min(abs(states - 23))
```

```{r}
no_policy <- numeric(length(states)) + 1

#set.seed(12345678)
set.seed(12345)

df <- mdp_planning(m_true, reward, discount, model_prior = c(1), 
                   policy = no_policy, x0 = x0, Tmax = Tmax)

df %>% mutate(state = states[state], action = actions[action]) %>% 
  ggplot(aes(time, state)) + geom_point()+geom_path() + 
  geom_line(aes(time, action), col="blue")
```




```{r}
reward <- array(dim=c(length(states), length(actions)))
for(i in 1:length(states)){
  for(j in 1:length(actions)){
    reward[i,j] <- reward_fn(states[i], actions[j])
  }
}
```



```{r}
belief_p <- mdp_value_iteration(m_belief, reward, discount, epsilon = 1e-4)
true_p <- mdp_value_iteration(m_true, reward, discount, epsilon = 1e-5)
#soln <- mdplearning::mdp_compute_policy(list(m_belief), reward, discount)
#opt_soln <- mdplearning::mdp_compute_policy(list(m_true), reward, discount)
```


Policy based on the belief (i.e. that system is bi-stable)

```{r}
tibble(state = states,
       belief = actions[belief_p$policy],
       ideal =  actions[true_p$policy]) %>%
  pivot_longer(c(belief, ideal), 
               names_to="model",
               values_to = "action") %>%
  ggplot(aes(state,action, color = model)) + geom_point() 
```





Result experienced by incorrect belief:  initial in-action followed by need for continued maintenance to prevent high-level outbreak:


```{r}
set.seed(12345)
df <- mdp_planning(m_true, reward, discount, model_prior = c(1), 
                   policy = belief_p$policy, x0 = x0, Tmax = 1000)

df %>% mutate(state = states[state], action = actions[action]) %>% 
  ggplot(aes(time, state)) + geom_point()+geom_path() + 
  geom_line(aes(time, action), col="blue")
```


Expected result based on the belief: stable low level is acceptable, so no action is required:

```{r}
set.seed(12345)
df <- mdp_planning(m_belief, reward, discount, model_prior = c(1), 
                   policy = belief_p$policy, x0 = 26, Tmax = 1000)

df %>% mutate(state = states[state], action = actions[action]) %>% 
  ggplot(aes(time, state)) + geom_point()+geom_path() + 
  geom_line(aes(time, action), col="blue")
```

Optimal strategy knowing this is just a transient (will depend on discount rate):

```{r}

df <- mdp_planning(m_true, reward, discount, model_prior = c(1), 
                   policy = true_p$policy, x0 = x0, Tmax = 1000)

df %>% mutate(state = states[state], action = actions[action]) %>% 
  ggplot(aes(time, state)) + geom_point()+geom_path() + 
  geom_line(aes(time, action), col="blue")
```





----

## Active Learning


```{r}

models <- map(possible_a, function(a){
  f <- may(a)
 transition_matrix(states, actions, f, p$sigma)
})
  
```

```{r}
transition <- models
```



## a near ghost

```{r}
wd <- sd(possible_a)
prior <- dnorm(possible_a, believe_a, wd/2)
prior <- prior / sum(prior)
```

```{r}
data.frame(a = possible_a, probability = prior) %>%
  ggplot(aes(a,prior)) + geom_bar(stat="identity") +
  geom_vline(aes(xintercept = true_a), col="red", lwd=1, lty=2) + 
  geom_vline(aes(xintercept = believe_a), col="blue", lwd=1, lty=2) 
```





```{r}
if(!file.exists("learning_sim.tsv.gz")){ # Cache the slow step

  learning_sim <- mdp_learning(transition, reward, discount, 
                      x0 = x0, 
                      Tmax = Tmax, 
                      true_transition = transition[[true_i]],
                      model_prior = prior,
                      type = "value iteration", 
                      epsilon = 1e-2)
  readr::write_tsv(learning_sim, "learning_sim.tsv.gz")
  
} else {
  learning_sim <- readr::read_tsv("learning_sim.tsv.gz")
}
```


```{r}
 learning_sim$df %>% 
  select(-value) %>% 
  gather(series, state, -time) %>% 
  ggplot(aes(time, states[state], color = series)) + geom_line()
```



```{r}
 learning_sim$posterior %>% 
  data.frame(time = 1:Tmax) %>%
  filter(time %in% seq(1,Tmax, by = 5)) %>%
  gather(param, probability, -time, factor_key =TRUE) %>% 
  mutate(param = as.numeric(param)) %>% 
  ggplot(aes(param, probability, group = time, alpha = time)) + 
  geom_line()
```


```{r}
```









-----


# Greta 

```{r}
library(greta)
```


```{r}
estimate_posterior <- function(df){
  X <- df$state
  # Reshape time-series data into ordered pairs
  n <- length(X)
  x_t1 <- X[-1]
  x_t <- X[-n] 
  
  # Prior
  a <- uniform(min(possible_a), max(possible_a))
  
  # Model definition
  mean <- x_t + p$r * x_t * (1 - x_t / p$K) - a * x_t ^ p$q / (x_t ^ p$q + p$b ^ p$q)
  distribution(x_t1) <- normal(mean, p$sigma * x_t)
  m <- model(a)

  # MCMC
  system.time({
    draws <- mcmc(m, n_samples = 1000, warmup = 3000, chains = 4, verbose = FALSE)
  })
  
  
  ## tidy
  samples <-  
    map_dfr(draws, function(x) data.frame(x, t = 1:dim(x)[1]), .id = "chain") %>% 
    gather(variable, value, -t, -chain)
}
```

```{r mcmc, cache=TRUE}
if(!file.exists("samples.tsv.gz")){ # Manually cache the slow step
  
samples <- examples %>% 
  group_by(series) %>% 
  group_modify(~estimate_posterior(.x))

readr::write_tsv(samples, "samples.tsv.gz")

} else {
  samples <- readr::read_tsv("samples.tsv.gz")
}
```


```{r, warning=FALSE, message=FALSE}
true <- tribble(~ value, ~ a,
                believe_a, "attractor",
                believe_a,  "attractor",
                true_a,     "ghost")

samples %>% 
  ggplot() + 
  geom_histogram(aes(value, fill=series), stat="density", alpha = 0.8) + 
  geom_vline(data = true, aes(xintercept = value, lty=a), col="black", lwd = 1) + 
  facet_wrap(~variable, scales = "free")
```



```{r}
```




